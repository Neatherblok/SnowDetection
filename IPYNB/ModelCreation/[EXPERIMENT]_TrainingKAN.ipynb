{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rdede\\AppData\\Local\\Temp\\ipykernel_22392\\1572454150.py:9: DeprecationWarning: c:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\ignite\\contrib\\handlers\\param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
      " Please refer to the documentation for more details.\n",
      "  from ignite.contrib.handlers.param_scheduler import LRScheduler\n",
      "c:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Data_Preparation.Preparation import CustomDataLoader\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.metrics import Recall, Loss\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.contrib.handlers.param_scheduler import LRScheduler\n",
    "import optuna\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyCNNModel(nn.Module):\n",
    "#     def __init__(self, n_filters, kernel_size):\n",
    "#         super(MyCNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, n_filters, kernel_size=kernel_size)\n",
    "#         self.conv2 = nn.Conv2d(n_filters, n_filters*2, kernel_size=kernel_size)\n",
    "#         self.conv3 = nn.Conv2d(n_filters*2, n_filters*4, kernel_size=kernel_size)\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv3(x), 2))\n",
    "#         x = self.flatten(x)\n",
    "#         flatten_shape = x.shape[1]\n",
    "#         self.fc1 = nn.Linear(flatten_shape, 50)  # Define the linear layer with the correct input size\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         self.fc2 = nn.Linear(50, 2)  # Define the output layer\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer_fn = optim.Adam\n",
    "\n",
    "# def objective(trial):\n",
    "#     lr = trial.suggest_float('lr', 0.001, 0.1)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32])\n",
    "#     num_epochs = trial.suggest_categorical('num_epochs', [5, 10, 15,20, 25, 30, 35, 40, 45, 50])\n",
    "#     n_filters = trial.suggest_categorical('n_filters', [4, 8, 16, 32])\n",
    "#     kernel_size = trial.suggest_categorical('kernel_size', [3, 5, 7])\n",
    "\n",
    "#     model = MyCNNModel(n_filters=n_filters, kernel_size=kernel_size)\n",
    "#     optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
    "\n",
    "#     train_loader = CustomDataLoader(data_path=\"./data\", batch_size=batch_size, dataset_type=\"train\").data_loader\n",
    "#     val_loader = CustomDataLoader(data_path=\"./data\", batch_size=batch_size, dataset_type=\"val\").data_loader\n",
    "\n",
    "#     trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "#     train_recall_metric = Recall()\n",
    "#     train_loss_metric = Loss(criterion)\n",
    "#     evaluator = create_supervised_evaluator(model, metrics={'recall': train_recall_metric, 'loss': train_loss_metric})\n",
    "\n",
    "#     @trainer.on(Events.EPOCH_COMPLETED)\n",
    "#     def compute_train_metrics(engine):\n",
    "#         train_recall_metric.reset()\n",
    "#         train_loss_metric.reset()\n",
    "#         for batch in train_loader:\n",
    "#             x, y = batch\n",
    "#             y_pred = model(x)\n",
    "#             loss = criterion(y_pred, y)\n",
    "#             train_recall_metric.update((y_pred, y))\n",
    "#             train_loss_metric.update((y_pred, y))  # Pass y_pred and y to update\n",
    "\n",
    "#     early_stopping = EarlyStopping(patience=3, score_function=lambda engine: torch.mean(train_recall_metric.compute()).item(), trainer=trainer)\n",
    "#     checkpoint = ModelCheckpoint(dirname='./checkpoints', filename_prefix='best', score_function=lambda engine: torch.mean(train_recall_metric.compute()).item(), score_name='recall', n_saved=1, require_empty=False)\n",
    "\n",
    "#     trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint, {'model': model, 'optimizer': optimizer})\n",
    "#     trainer.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)\n",
    "\n",
    "#     trainer.run(train_loader, max_epochs=num_epochs)\n",
    "#     evaluator.run(val_loader)\n",
    "#     recall = train_recall_metric.compute()\n",
    "#     print(f'Validation Recall: {torch.mean(recall).item():.4f}')\n",
    "    \n",
    "#     return {'recall': torch.mean(recall).item()}\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# best_hyperparams = study.best_params\n",
    "# best_metric_value = study.best_value\n",
    "\n",
    "# print(f'Best hyperparameters: {best_hyperparams}')\n",
    "# print(f'Best metric value: {best_metric_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN\n",
    "\n",
    "def create_kan():\n",
    "    return KAN(width=[128**2, 1, 2], grid=3, k=3)\n",
    "model = create_kan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kan import KAN\n",
    "\n",
    "# class MyCNNModel(nn.Module):\n",
    "#     def __init__(self, kernel_size):\n",
    "#         super(MyCNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 128, kernel_size=kernel_size)\n",
    "#         self.conv2 = nn.Conv2d(128, 256, kernel_size=kernel_size)\n",
    "#         self.conv3 = nn.Conv2d(256, 512, kernel_size=kernel_size)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.kan = KAN(width=[73728,20,2], grid=3, k=3, seed=0,device='cuda:0')\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n",
    "#         x = torch.relu(torch.max_pool2d(self.conv3(x), 2))\n",
    "#         x = self.flatten(x)\n",
    "#         out = self.kan(x)\n",
    "#         return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 0.01\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 25\n",
    "N_FILTERS = 4\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataLoader(data_path=\"./data\", batch_size=BATCH_SIZE, dataset_type=\"train\")\n",
    "val_set = CustomDataLoader(data_path=\"./data\", batch_size=BATCH_SIZE, dataset_type=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_set) // BATCH_SIZE\n",
    "valSteps = len(val_set) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc():\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.argmax(model(val_set.dataloader[0]), dim=1)\n",
    "        correct = (predictions == val_set.dataloader[1]).float()\n",
    "        accuracy = correct.mean()\n",
    "    return accuracy\n",
    "\n",
    "def train_acc():\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.argmax(model(train_set.dataloader[0].cuda()), dim=1)\n",
    "        correct = (predictions == train_set.dataloader[1].cuda()).float()\n",
    "        accuracy = correct.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "train_inputs = []\n",
    "train_labels = []\n",
    "for i, data in enumerate(train_set.dataset):\n",
    "    image, label = data\n",
    "    train_inputs.append(torch.flatten(image, start_dim=1).long().cuda())\n",
    "    train_labels.append(torch.tensor(label).cuda())\n",
    "\n",
    "test_inputs = []\n",
    "test_labels = []\n",
    "for i, data in enumerate(val_set.dataset):\n",
    "    image, label = data\n",
    "    test_inputs.append(torch.flatten(image, start_dim=1).long().cuda())\n",
    "    test_labels.append(torch.tensor(label).cuda())\n",
    "\n",
    "dataset['train_input'] = torch.stack(train_inputs)\n",
    "dataset['train_label'] = torch.stack(train_labels)\n",
    "dataset['test_input'] = torch.stack(test_inputs)\n",
    "dataset['test_label'] = torch.stack(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.5351909170230562\n",
      "std=0.20247474194162862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Set the path to your local image folder\n",
    "image_folder = './data'\n",
    "\n",
    "# Define a custom dataset class to load images from local folder\n",
    "class LocalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        label_dict = {}\n",
    "        for idx, label in enumerate(os.listdir(image_folder)):\n",
    "            label_dict[label] = idx\n",
    "            for file in os.listdir(os.path.join(image_folder, label)):\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):  # adjust file extensions as needed\n",
    "                    self.images.append(os.path.join(image_folder, label, file))\n",
    "                    self.labels.append(label_dict[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 128))  # resize to fixed shape\n",
    "        img = img[12:116, 12:116]  # center crop to fixed shape\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Create dataset and data loader for training and testing\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = LocalImageDataset(os.path.join(image_folder, 'train'), transform=transform)\n",
    "test_dataset = LocalImageDataset(os.path.join(image_folder, 'test'), transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for img, label in train_loader:\n",
    "    img = img.numpy().squeeze()  # convert tensor to numpy array\n",
    "    for i in range(img.shape[0]):\n",
    "        img_resized = cv2.resize(img[i], (128, 128))  # resize to fixed shape\n",
    "        X_train.append(img_resized.astype(float))\n",
    "        y_train.append(label[0])\n",
    "\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Calculate mean and std of training data\n",
    "mean, std = np.mean(X_train), np.std(X_train)\n",
    "print(f\"{mean=}\")\n",
    "print(f\"{std=}\")\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for img, label in test_loader:\n",
    "    img = img.numpy().squeeze()  # convert tensor to numpy array\n",
    "    for i in range(img.shape[0]):\n",
    "        img_resized = cv2.resize(img[i], (128, 128))  # resize to fixed shape\n",
    "        X_test.append(img_resized.astype(float))\n",
    "        y_test.append(label[0])\n",
    "\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Normalize testing data using mean and std of training data\n",
    "X_test = (X_test - mean) / std\n",
    "X_train = (X_train - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "dataset[\"train_input\"] = (\n",
    "    torch.from_numpy(np.array([data[0] for data in train_set])).long().to(device)\n",
    ")\n",
    "dataset[\"train_label\"] = torch.from_numpy(np.array([data[1] for data in train_set])).long().to(device)\n",
    "\n",
    "dataset[\"test_input\"] = (\n",
    "    torch.from_numpy(np.array([data[0] for data in val_set])).long().to(device)\n",
    ")\n",
    "dataset[\"test_label\"] = torch.from_numpy(np.array([data[1] for data in val_set])).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 78\n",
    "height = 128\n",
    "width = 128\n",
    "channels = 3\n",
    "\n",
    "# Assuming your image batch is a tensor with shape (batch_size, channels, height, width)\n",
    "image_batch = dataset[\"train_label\"]\n",
    "\n",
    "# Reshape the image batch to have shape (batch_size, features)\n",
    "dataset[\"train_label\"] = image_batch.view(batch_size, -1)\n",
    "\n",
    "#dataset[\"train_input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, custom_dataset):\n",
    "        self.custom_dataset = custom_dataset\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key == 'train_input':\n",
    "            return [img for img, _ in self.custom_dataset]\n",
    "        elif key == 'test_input':\n",
    "            return [label for _, label in self.custom_dataset]\n",
    "        else:\n",
    "            raise KeyError(f\"Unknown key {key}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.custom_dataset)\n",
    "    \n",
    "dataset = train_set.dataset\n",
    "dataset = CustomDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|                                                           | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 16384]' is invalid for input of size 786432",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#dataset['train_input'] = dataset['train_input'].unsqueeze(1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLBFGS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_acc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\KAN.py:898\u001b[0m, in \u001b[0;36mKAN.train\u001b[1;34m(self, dataset, opt, steps, log, lamb, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff, update_grid, grid_update_num, loss_fn, lr, stop_grid_update_step, batch, small_mag_threshold, small_reg_factor, metrics, sglr_avoid, save_fig, in_vars, out_vars, beta, save_fig_freq, img_folder, device)\u001b[0m\n\u001b[0;32m    895\u001b[0m test_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_input\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size_test, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;241m%\u001b[39m grid_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _ \u001b[38;5;241m<\u001b[39m stop_grid_update_step \u001b[38;5;129;01mand\u001b[39;00m update_grid:\n\u001b[1;32m--> 898\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_grid_from_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBFGS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    901\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\KAN.py:243\u001b[0m, in \u001b[0;36mKAN.update_grid_from_samples\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03mupdate grid from samples\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03mtensor([0.0128, 1.0064, 2.0000, 2.9937, 3.9873, 4.9809])\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fun[l]\u001b[38;5;241m.\u001b[39mupdate_grid_from_samples(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts[l])\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\KAN.py:311\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts\u001b[38;5;241m.\u001b[39mappend(x)  \u001b[38;5;66;03m# acts shape: (batch, width[l])\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m--> 311\u001b[0m     x_numerical, preacts, postacts_numerical, postspline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_enabled \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m         x_symbolic, postacts_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_fun[l](x)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\KANLayer.py:170\u001b[0m, in \u001b[0;36mKANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    168\u001b[0m batch \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# x: shape (batch, in_dim) => shape (size, batch) (size = out_dim * in_dim)\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,k->ikj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    171\u001b[0m preacts \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)\n\u001b[0;32m    172\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_fun(x)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (batch, size)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 16384]' is invalid for input of size 786432"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"[INFO] Start training\")\n",
    "print(\"---------------------\")\n",
    "startTime = time.time()\n",
    "\n",
    "#dataset['train_input'] = dataset['train_input'].unsqueeze(1)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    dataset,\n",
    "    opt=\"LBFGS\",\n",
    "    steps=EPOCHS,\n",
    "    batch=16,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    metrics=(train_acc, test_acc)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the CNN model...\n"
     ]
    }
   ],
   "source": [
    "# from torch.optim import Adam\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# # initialize the CatSpotter model\n",
    "# print(\"[INFO] initializing the CNN model...\")\n",
    "# model = MyCNNModel(\n",
    "#     # n_filters=N_FILTERS, \n",
    "#     kernel_size=KERNEL_SIZE\n",
    "#     ).cuda()\n",
    "# # initialize our optimizer and loss function\n",
    "# opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # initialize a dictionary to store training history\n",
    "# H = {\n",
    "#     \"train_loss\": [],\n",
    "#     \"train_acc\": [],\n",
    "#     \"val_loss\": [],\n",
    "#     \"val_acc\": []\n",
    "# }\n",
    "# # Create a GradScaler for mixed-precision training\n",
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model checkpoint saving location\n",
    "PATH = 'models/04-26-2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "def save_ckp(state, checkpoint_dir, epoch_nr):\n",
    "    f_path = checkpoint_dir + f'/epoch_{epoch_nr}.pt'\n",
    "    torch.save(state, f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [06:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#X = X.view(X.size(0),-1).cuda()\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, Y\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mMyCNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x), \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m---> 17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\KAN.py:314\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    311\u001b[0m x_numerical, preacts, postacts_numerical, postspline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fun[l](x)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_enabled \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     x_symbolic, postacts_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     x_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\kan\\Symbolic_KANLayer.py:109\u001b[0m, in \u001b[0;36mSymbolic_KANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m postacts_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim):\n\u001b[1;32m--> 109\u001b[0m     xij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine[j,i,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuns[j][i](\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine[j,i,\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    110\u001b[0m     postacts_\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[j][i]\u001b[38;5;241m*\u001b[39mxij)\n\u001b[0;32m    111\u001b[0m postacts\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack(postacts_))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"[INFO] Start training\")\n",
    "print(\"---------------------\")\n",
    "startTime = time.time()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    total_batch = len(train_set.dataset)//BATCH_SIZE\n",
    "    \n",
    "    # Training phase\n",
    "    #model.train()  # Set the model to train mode\n",
    "\n",
    "    # Get statistics\n",
    "    epoch_loss = 0\n",
    "    len_dataset = 0\n",
    "    \n",
    "    for step, (batch_images, batch_labels) in enumerate(train_set,0):\n",
    "        X, Y = batch_images, batch_labels\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        #X = X.view(X.size(0),-1).cuda()\n",
    "        pred = model(X.cuda())\n",
    "        loss = criterion(pred, Y.cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += pred.shape[0] * loss.item()\n",
    "        len_dataset += pred.shape[0]\n",
    "        if (step) % 10 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, EPOCHS, step+1, total_batch, loss.item()))\n",
    "\n",
    "    epoch_loss = epoch_loss/ len_dataset\n",
    "    print('Epoch: ', epoch + 1, '| train loss : %0.4f' % epoch_loss)\n",
    "\n",
    "    # # Save the epoch model parameters\n",
    "    # checkpoint = {\n",
    "    #     'epoch': epoch + 1,\n",
    "    #     'state_dict': model.state_dict(),\n",
    "    #     'optimizer': opt.state_dict()\n",
    "    # }\n",
    "\n",
    "    # save_ckp(checkpoint, PATH , epoch)\n",
    "\n",
    "    \n",
    "    # Validation phase\n",
    "    #model.eval()  # Set the model to evaluation mode\n",
    "    with torch.inference_mode():  \n",
    "        running_loss = 0\n",
    "        for step, (images, labels) in enumerate(val_set):\n",
    "            \n",
    "            #x = images.view(images.size(0),-1).cuda()\n",
    "\n",
    "            outputs = model(images.cuda())\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    running_loss = running_loss / len(val_set.dataset)\n",
    "    print('Epoch: ', epoch +1, '| val loss : %0.4f' % running_loss )\n",
    "\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "    endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.50      1.00      0.67         9\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.25      0.50      0.33        18\n",
      "weighted avg       0.25      0.50      0.33        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rdede\\Documents\\GitHub\\SnowDetection\\.env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    #model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for batch in val_set:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0),-1).cuda()\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        labels.extend(y.cpu().numpy())\n",
    "        \n",
    "print(preds, labels)\n",
    "\n",
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] evaluating network...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# turn off autograd for testing evaluation\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      9\u001b[0m     preds \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for batch in train_set:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        labels.extend(y.cpu().numpy())\n",
    "\n",
    "print(classification_report(labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
